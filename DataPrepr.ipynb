{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPrepr.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShykerBogdan/Prozzoro_project/blob/master/DataPrepr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5NNgY4mELDf6",
        "colab_type": "code",
        "outputId": "0b808e63-2b88-4771-947f-14d4fcb861c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZNSTK99IXPll",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_rqhjuhLMBb",
        "colab_type": "code",
        "outputId": "0aa50606-07f7-4933-aa4b-6c4dd1f4a126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "filename='/content/gdrive/My Drive/ML_proz/data_source.csv'\n",
        "df=pd.read_csv(filename)\n",
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,5,14,15,16,17,18,20,24,38,48,50,51,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3507778, 56)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "xF8VfObanZNz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class DataPreproccessing:\n",
        "    def __init__(self, data_source, churn_interval, start_period=None, end_period=None):\n",
        "        self.last_day=pd.to_datetime('2018-11-29')\n",
        "        # для статистики постачальника вибираємо проміжок [start_period,end_period]\n",
        "        self.divide_data = self.last_day - pd.DateOffset(days=churn_interval)\n",
        "        print(self.divide_data)\n",
        "        self.data_source = data_source[pd.to_datetime(data_source['date_from_id']) <= self.divide_data]\n",
        "        # для labeling вибираємо період [end_period, end_period+churn_interval]\n",
        "        self.label_data = data_source[pd.to_datetime(data_source['date_from_id']) > self.divide_data]\n",
        "        # вибираємо унікальних постачальників\n",
        "        self.unique_id = list(self.data_source['participants'].value_counts().index)\n",
        "        # створюємо dataframe з колонкою унікальних постачальників і туди будемо додавати features\n",
        "        self.feature_data = pd.DataFrame(self.unique_id, columns=['unique_id'])\n",
        "        print(self.feature_data.iloc[:10]['unique_id'].values)\n",
        "     \n",
        "    def cleaning(self):\n",
        "        self.data_source['winner']=self.data_source[['contracts.value_amount','contracts.status','winner']].apply(lambda x: 0 if x['contracts.value_amount']=='\\\\N'\n",
        "                                                                                                         or x['contracts.status']=='cancelled' else x['winner'],axis=1  )\n",
        "        print('winner updated')\n",
        "        self.activity_date()\n",
        "        print('activity date')\n",
        "    def split_into_batches(self,n=20000,filename=None):\n",
        "        self.cleaning()\n",
        "        feature_batches=list()\n",
        "        n_iter=int(np.ceil(self.feature_data.shape[0]/n))\n",
        "        data=self.data_source[['participants','new_date']]\n",
        "        for i in range(n_iter):\n",
        "          print('Batch {0}-{1}:'.format(i*n,(i+1)*n))\n",
        "          unique_id=self.feature_data.iloc[i*n:(i+1)*n]['unique_id'].values\n",
        "          feature_df=pd.DataFrame(unique_id,columns=['unique_id'])\n",
        "          feature_batches.append(feature_df)\n",
        "          # part of data_source\n",
        "          df_=data.loc[data['participants'].isin(unique_id)]\n",
        "          print('Unique_supp:{0}-{1}'.format(len(unique_id),df_.shape[0]))\n",
        "          df_=df_.sort_values(by='new_date')\n",
        "          def get_average_activity(raw,data):\n",
        "            dates_=list(df_[df_['participants']==raw['unique_id']]['new_date'].values)\n",
        "            try:\n",
        "              raw['average_activity']=(pd.to_datetime(dates_[-1])-pd.to_datetime(dates_[0])).days/len(dates_)\n",
        "            except:\n",
        "              raw['average_activity']=np.nan\n",
        "            return raw\n",
        "          feature_batches[i]=feature_batches[i].apply(get_average_activity,args=(df_,),axis=1)\n",
        "          if filename:\n",
        "            print(f'write to csv')\n",
        "            feature_batches[i].to_csv(filename+ '{0}'.format(i)+'.csv',index=False)\n",
        "        # concat dataframes into one\n",
        "        print('Concat')\n",
        "        all_batches_df = pd.concat(feature_batches)\n",
        "        if filename:\n",
        "          print('write to csv_all')\n",
        "          all_batches_df.to_csv(filename+'_all.csv',index=False)\n",
        "          \n",
        "          \n",
        "        \n",
        "    \n",
        "    def get_label(self):\n",
        "        participants=list(self.label_data['participants'].value_counts().index)\n",
        "        \n",
        "        print(len(participants))\n",
        "        def get_y(raw,participants):\n",
        "          if raw['unique_id'] in participants:\n",
        "            raw['y']=1\n",
        "          else:\n",
        "            raw['y']=0\n",
        "          return raw\n",
        "        \n",
        "        self.feature_data=self.feature_data.apply(get_y,args=(participants,),axis=1)\n",
        "        \n",
        "    def graph_2(self,filename=None):\n",
        "        self.data_source['winner']=self.data_source[['contracts.value_amount','contracts.status','winner']].apply(lambda x: 0 if x['contracts.value_amount']=='\\\\N'\n",
        "                                                                                                         or x['contracts.status']=='cancelled' else x['winner'],axis=1  )\n",
        "\n",
        "        self.activity_date()\n",
        "        print('activity date')\n",
        "        \n",
        "        data=self.data_source[['participants','new_date']]\n",
        "        data=data.sort_values(by='new_date')\n",
        "        def get_average_activity(raw,data):\n",
        "          dates_=list(data[data['participants']==raw['unique_id']]['new_date'].values)\n",
        "          try:\n",
        "            raw['average_activity']=(pd.to_datetime(dates_[-1])-pd.to_datetime(dates_[0])).days/len(dates_)\n",
        "          except:\n",
        "            raw['average_activity']=np.nan\n",
        "          return raw\n",
        "        \n",
        "        self.feature_data=self.feature_data.apply(get_average_activity,args=(data,),axis=1)\n",
        "        print('average_activity')\n",
        "        if filename:\n",
        "          print('write to csv')\n",
        "          self.feature_data.to_csv(filename)\n",
        "    \n",
        "    def create_features(self,filename=None):\n",
        "        self.get_label()\n",
        "        print('get y')\n",
        "        self.data_source['winner']=self.data_source[['contracts.value_amount','contracts.status','winner']].apply(lambda x: 0 if x['contracts.value_amount']=='\\\\N'\n",
        "                                                                                                         or x['contracts.status']=='cancelled' else x['winner'],axis=1  )\n",
        "        print('winner updated')\n",
        "        self.activity_date()\n",
        "        print('activity date')\n",
        "        \n",
        "#         self.average_economy()\n",
        "#         print('average economy')\n",
        "        self.count_lots()\n",
        "        print('count_lots has already finish')\n",
        "        self.count_win_lose()\n",
        "        print('count_win_lose has already finish')\n",
        "#         self.count_win_type_procedure()\n",
        "#         print('count_win_type_procedure has already finish')\n",
        "#         self.count_lose_type_procedure()\n",
        "#         print('count_lose_type_procedure has already finish')\n",
        "#         self.feature_data['count_lots_open']=self.feature_data['win_open']+self.feature_data['lose_open']\n",
        "#         self.feature_data['count_lots_not_open']=self.feature_data['win_not_open']+self.feature_data['lose_not_open']\n",
        "#         print('count_lots_open has already finish')\n",
        "        self.last_activity()\n",
        "        print('last activity')\n",
        "#         self.average_activity()\n",
        "#         print('average activity')\n",
        "        \n",
        "        \n",
        "        if filename:\n",
        "          self.feature_data.to_csv(filename)\n",
        "          \n",
        "    def count_lots(self):\n",
        "        freq = self.data_source['participants'].value_counts()\n",
        "\n",
        "        def count_lots(raw, frequency):\n",
        "            raw['count_lots'] = frequency[raw['unique_id']]\n",
        "            return raw\n",
        "\n",
        "        self.feature_data = self.feature_data.apply(count_lots, args=(freq,), axis=1)\n",
        "\n",
        "    def count_win_lose(self):\n",
        "        data=self.data_source[self.data_source['winner']==1]\n",
        "        frequency=data['participants'].value_counts()\n",
        "        \n",
        "        def win_lose(raw,frequency):\n",
        "            # use try bsc if index not in frequency 'win'=0\n",
        "            try:\n",
        "              raw['win']=frequency[raw['unique_id']]\n",
        "            except:\n",
        "              raw['win']=0\n",
        "            return raw\n",
        "            \n",
        "\n",
        "        self.feature_data = self.feature_data.apply(win_lose, args=(\n",
        "        frequency,),axis=1)\n",
        "        self.feature_data['lose']=self.feature_data['count_lots']-self.feature_data['win']\n",
        "        \n",
        "    # всьо в одну функцію\n",
        "    def count_win_type_procedure(self):\n",
        "        data=self.data_source[self.data_source['winner']==1]\n",
        "        data=data[data['tenders.procurementMethod']=='open']\n",
        "        frequency=data['participants'].value_counts()\n",
        "        def count_win_open(raw,frequency):\n",
        "            try:\n",
        "              raw['win_open']=frequency[raw['unique_id']]\n",
        "            except:\n",
        "              raw['win_open']=0\n",
        "            return raw\n",
        "              \n",
        "        self.feature_data = self.feature_data.apply(count_win_open, args=(\n",
        "        frequency,),axis=1)\n",
        "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "        self.feature_data['win_not_open']=self.feature_data['win']-self.feature_data['win_open']\n",
        "    \n",
        "    \n",
        "    def count_lose_type_procedure(self):\n",
        "        data=self.data_source[self.data_source['winner']==0]\n",
        "        data=data[data['tenders.procurementMethod']=='open']\n",
        "        frequency=data['participants'].value_counts()\n",
        "        def count_lose_open(raw,frequency):\n",
        "            try:\n",
        "              raw['lose_open']=frequency[raw['unique_id']]\n",
        "            except:\n",
        "              raw['lose_open']=0\n",
        "            return raw\n",
        "              \n",
        "        self.feature_data = self.feature_data.apply(count_lose_open, args=(\n",
        "        frequency,),axis=1)\n",
        "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "        self.feature_data['lose_not_open']=self.feature_data['lose']-self.feature_data['lose_open']\n",
        "    \n",
        "    \n",
        "    def activity_date(self):\n",
        "        f=['winner','tenders.auctionPeriod_startDate','contracts.dateSigned','date_from_id','awards.date']\n",
        "        self.data_source['new_date']=self.data_source[f].apply(lambda x:x[f[2]] if x[f[0]==1] else (x[f[3]] if pd.isnull(x[f[1]]) else x[f[1]] ),axis=1)\n",
        "        self.data_source['new_date']=self.data_source[['new_date']].apply(lambda x: x['new_date'][:10],axis=1)\n",
        "        self.data_source['new_date']=self.data_source[['new_date','date_from_id']].apply(lambda x: x['date_from_id'] if x['new_date']=='\\\\N' else x['new_date'],axis=1)\n",
        "        \n",
        "        \n",
        "    def last_activity(self):\n",
        "        data=self.data_source[['participants','new_date']]\n",
        "        data=data.sort_values(by='new_date')\n",
        "        self.divide_data=str(self.divide_data)[:10]\n",
        "        \n",
        "        def get_last_activity(raw,data):\n",
        "          raw['last_activity_date']=data[data['participants']==raw['unique_id']].iloc[-1]['new_date']\n",
        "          try:\n",
        "            raw['last_activity_days']=(pd.to_datetime(self.divide_data)-pd.to_datetime(raw['last_activity_date'])).days\n",
        "          except:\n",
        "            print('--------------')\n",
        "            raw['last_activity_days']='?'\n",
        "          return raw\n",
        "        \n",
        "        \n",
        "        self.feature_data=self.feature_data.apply(get_last_activity,args=(data,),axis=1)\n",
        "        self.feature_data['last_activity_days']=self.feature_data.apply(lambda x: 0 if x['last_activity_days']<0 else x['last_activity_days'],axis=1)\n",
        "        \n",
        "    \n",
        "    \n",
        "    def average_activity(self):\n",
        "        data=self.data_source[['participants','new_date']]\n",
        "        data=data.sort_values(by='new_date')\n",
        "        def get_average_activity(raw,data):\n",
        "          dates=list(data[data['participants']==raw['unique_id']]['new_date'].values)\n",
        "#           dates=np.diff([pd.to_datetime(item) for item in dates])\n",
        "\n",
        "#           dates=[item.days for item in dates]\n",
        "#           dates.append(raw['last_activity_days'])\n",
        "          raw['average_activity']=np.average(dates)\n",
        "          return raw\n",
        "    \n",
        "        self.feature_data=self.feature_data.apply(get_average_activity,args=(data,),axis=1)\n",
        "     \n",
        "    def average_economy(self):\n",
        "        def convert_to_float(raw):\n",
        "          try:\n",
        "            raw['lots.value_amount']=float(raw['lots.value_amount'])\n",
        "          except:\n",
        "            raw['lots.value_amount']=np.nan\n",
        "          return raw\n",
        "        self.data_source['lots.value_amount']=self.data_source[['lots.value_amount']].apply(convert_to_float,axis=1)\n",
        "        print('convert lots')\n",
        "        data=self.data_source[self.data_source['winner']==1][['participants','awards.value_amount','lots.value_amount']]\n",
        "        data['economy_value']=data['lots.value_amount']-data['awards.value_amount']\n",
        "        data['economy_percent']=data['awards.value_amount']/data['lots.value_amount']\n",
        "        def get_economy(raw,data):\n",
        "          d=data[data['participants']==raw['unique_id']]\n",
        "          raw['economy_value']=np.nanmean(d['economy_value'])\n",
        "          raw['economy_percent']=np.nanmean(d['economy_percent'])\n",
        "#           raw['economy_value']=np.mean(d['lots.value_amount']-d['awards.value_amount'])\n",
        "#           raw['economy_percent']=np.mean(d['awards.value_amount']/d['lots.value_amount'])\n",
        "          return raw\n",
        "        self.feature_data=self.feature_data.apply(get_economy,args=(data,),axis=1)\n",
        "          \n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWxrHqsVHrKM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "col=['winner','tenders.auctionPeriod_startDate','contracts.dateSigned',\n",
        "     'awards.date','participants','date_from_id','contracts.value_amount','contracts.status']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s149uQR1hvbp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "PRJnlfwtMNP4",
        "colab_type": "code",
        "outputId": "70aa674c-02dd-4197-ff4c-f6a054dac465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1737
        }
      },
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "df1=df[col]\n",
        "prepr = DataPreproccessing(df1,1)\n",
        "prepr.split_into_batches(n=10000,filename='try')\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(prepr.feature_data.head())\n",
        "# 158 sec - 30000"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-28 00:00:00\n",
            "['25394112' '32490244' '39273420' '41449359' '40473930' '36248687'\n",
            " '21633086' '21560766' '39190161' '33680859']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "winner updated\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:191: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:192: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:193: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "activity date\n",
            "Batch 0-10000:\n",
            "Unique_supp:10000-2021555\n",
            "write to csv\n",
            "Batch 10000-20000:\n",
            "Unique_supp:10000-435477\n",
            "write to csv\n",
            "Batch 20000-30000:\n",
            "Unique_supp:10000-259041\n",
            "write to csv\n",
            "Batch 30000-40000:\n",
            "Unique_supp:10000-174904\n",
            "write to csv\n",
            "Batch 40000-50000:\n",
            "Unique_supp:10000-126328\n",
            "write to csv\n",
            "Batch 50000-60000:\n",
            "Unique_supp:10000-95018\n",
            "write to csv\n",
            "Batch 60000-70000:\n",
            "Unique_supp:10000-73963\n",
            "write to csv\n",
            "Batch 70000-80000:\n",
            "Unique_supp:10000-57168\n",
            "write to csv\n",
            "Batch 80000-90000:\n",
            "Unique_supp:10000-46246\n",
            "write to csv\n",
            "Batch 90000-100000:\n",
            "Unique_supp:10000-38369\n",
            "write to csv\n",
            "Batch 100000-110000:\n",
            "Unique_supp:10000-30000\n",
            "write to csv\n",
            "Batch 110000-120000:\n",
            "Unique_supp:10000-25296\n",
            "write to csv\n",
            "Batch 120000-130000:\n",
            "Unique_supp:10000-20000\n",
            "write to csv\n",
            "Batch 130000-140000:\n",
            "Unique_supp:10000-20000\n",
            "write to csv\n",
            "Batch 140000-150000:\n",
            "Unique_supp:10000-14544\n",
            "write to csv\n",
            "Batch 150000-160000:\n",
            "Unique_supp:10000-10000\n",
            "write to csv\n",
            "Batch 160000-170000:\n",
            "Unique_supp:10000-10000\n",
            "write to csv\n",
            "Batch 170000-180000:\n",
            "Unique_supp:10000-10000\n",
            "write to csv\n",
            "Batch 180000-190000:\n",
            "Unique_supp:10000-10000\n",
            "write to csv\n",
            "Batch 190000-200000:\n",
            "Unique_supp:10000-10000\n",
            "write to csv\n",
            "Batch 200000-210000:\n",
            "Unique_supp:10000-10000\n",
            "write to csv\n",
            "Batch 210000-220000:\n",
            "Unique_supp:5874-5874\n",
            "write to csv\n",
            "Concat\n",
            "write to csv_all\n",
            "--- 8540.415004730225 seconds ---\n",
            "  unique_id\n",
            "0  25394112\n",
            "1  32490244\n",
            "2  39273420\n",
            "3  41449359\n",
            "4  40473930\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}